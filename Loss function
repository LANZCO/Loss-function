import torch
import torch.nn as nn
import torch.nn.functional as F

input = torch.randn(3, requires_grad=True)  # 从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的3个随机数
target = torch.empty(3).random_(2)  # 生成3个值，值为0 或者 1

#二值交叉熵，这里输入要经过sigmoid处理
out = F.sigmoid(input)
loss = nn.BCELoss(out, target)

#多分类交叉熵, 用这个 loss 前面不需要加 Softmax 层
out = activation fuction(input)#activation fuction是激活函数
loss= nn.CrossEntropyLoss(input, target)

loss.backward()
nn.CrossEntropyLoss(input, target, weight=class_weight)
import torch
Loss = nn.MSELoss()
input = torch.randn(3, 5, requires_grad=True)
target = torch.randn(3, 5)
loss = loss(input, target)
loss.backward()
import torch.nn as nn
import torch.nn.functional as F

class SoftDiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(SoftDiceLoss, self).__init__()
    def forward(self, logits, targets):
        num = targets.size(0)
        smooth = 1
        probs = F.sigmoid(logits)
        m1 = probs.view(num, -1)
        m2 = targets.view(num, -1)
        intersection = (m1 * m2)
        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)
        score = 1 - score.sum() / num
        return score
        import torch
import torch.nn as nn

#二分类
class FocalLoss(nn.Module):

    def __init__(self, gamma=2,alpha=0.25):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha=alpha
    def forward(self, input, target):
        # input:size is M*2. M　is the batch　number
        # target:size is M.
        pt=torch.softmax(input,dim=1)
        p=pt[:,1]
        loss = -self.alpha*(1-p)**self.gamma*(target*torch.log(p))-\
               (1-self.alpha)*p**self.gamma*((1-target)*torch.log(1-p))
        return loss.mean()
